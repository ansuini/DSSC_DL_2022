{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Basics of Autograd in Python (and first overview of _backpropagation_)\n",
    "\n",
    "What did we see last time?\n",
    "\n",
    "- PyTorch basics\n",
    "- Construction of a multilayer perceptron using PyTorch API\n",
    "\n",
    "**What caught our attention?**\n",
    "\n",
    "![](imgs/02/grad.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation in PyTorch\n",
    "\n",
    "PyTorch is built with support for differentiation in mind.\n",
    "In the end, Deep Learning (for now) is all about differentiation and building cascades of differentiable function into complicated multilayer deep neural networks.\n",
    "\n",
    "Essentially, all PyTorch built-ins support differentiability (unless the function is not differentiable, of course).\n",
    "Today we will see how to compute derivatives in PyTorch.\n",
    "Also, we will learn how to create differentiable modules using PyTorch APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation and recall\n",
    "\n",
    "1. **Function** $f:\\mathbb{R}\\rightarrow\\mathbb{R}$, given $x\\in\\mathbb{R}$, derivative is $\\frac{\\partial f}{\\partial x}$\n",
    "2. **Scalar function** $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$, we have a vector $\\mathbf{x}\\in\\mathbb{R}^d = (x_1,\\dots,x_d)$, we calculate the derivative of $f$ w.r.t. each of the dimensions of $\\mathbf{x}$ and obtain the gradient $\\nabla_f = (\\frac{\\partial f}{\\partial x_1},\\dots,\\frac{\\partial f}{\\partial x_d})$\n",
    "3. **Vector function** $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}^k$, given $\\mathbf{x}$, we have $f(\\mathbf{x})=(f_1(\\mathbf{x}),\\dots,f_k(\\mathbf{x}))$, hence we can calculate $k$ gradients which we can gather in the Jacobian: $J_f=\\begin{pmatrix}\\frac{\\partial f_1}{\\partial x_1} & \\dots & \\frac{\\partial f_1}{\\partial x_d}\\\\\\vdots&\\ddots&\\vdots\\\\\\frac{\\partial f_k}{\\partial x_1} & \\dots & \\frac{\\partial f_k}{\\partial x_d}\\end{pmatrix} \\in \\mathbb{R}^{d\\times k}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grad functionality\n",
    "\n",
    "\"Under-the-hood\", each PT Tensor has an attribute `requires_grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(3,3)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually set this to `True` or create directly a Tensor supporting grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(3, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1\n",
    "\n",
    "Suppose we are in case 1.: $f:\\mathbb{R}\\rightarrow\\mathbb{R}$.\n",
    "\n",
    "For instance, $f(x) = x^2$.\n",
    "\n",
    "We could apply $f$ to a singleton tensor and calculate the derivative.\n",
    "\n",
    "We expect the derivative to be... ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, requires_grad=True)\n",
    "\n",
    "print(\"x:\", x)\n",
    "\n",
    "y = x**2\n",
    "\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradient, we call `backward()` on the Tensor. Which one, `x` or `y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "something.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the gradient of x\n",
    "by accessing its grad attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that it's correct..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad == 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, when there's no gradient, it is automatically set to `None` to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(3,3).grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 (scalar function)\n",
    "\n",
    "We can use the same `.backward()` call to get the gradient of a scalar function.\n",
    "\n",
    "Now x will be a vector (or a matrix, it doesn't really matter for our case) and we will apply to it a function which returns a single scalar.\n",
    "\n",
    "One example may be $f(\\mathbf{x})=\\sum_{i=1}^d x_i$.\n",
    "\n",
    "**Q**: What is the gradient we expect to obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([5], requires_grad=True)\n",
    "\n",
    "y = x.sum()\n",
    "\n",
    "y.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3 (vector function)\n",
    "\n",
    "Unfortunately, the backward computation of the gradient is not directly capable of calculating the gradient for a vector of values, but only for a single scalar.\n",
    "\n",
    "If we wanted to compute the gradient on a vector function, what could we do?\n",
    "\n",
    "1. There exist a forward differentiation, which is not though present in PT\n",
    "2. Using PT backward functionality... (complete as homework)\n",
    "\n",
    "**Q**: Why is really the backward differentiation (and not the forward) useful for our case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composition of functions\n",
    "\n",
    "We can use also `backward` to compute the gradient of a composition of functions. For our objective, it will be very useful to think in terms of computational graph.\n",
    "\n",
    "We can view $y=g(f(x))$ as\n",
    "\n",
    "![](imgs/02/compgra1.jpg)\n",
    "\n",
    "We might extend this and add a hidden node $z$\n",
    "between $f$ and $g$\n",
    "\n",
    "![](imgs/02/compgra2.jpg)\n",
    "\n",
    "Supposing $f(x)=log(x)$\n",
    "and $g(x)=x^2$, we can reproduce this example in PyTorch.\n",
    "\n",
    "**Q**\n",
    "\n",
    "- What we expect to get from $\\partial g/\\partial z$?\n",
    "\n",
    "- And from $\\partial f/\\partial x$?\n",
    "\n",
    "- And from $\\partial g/\\partial x$?\n",
    "\n",
    "- More specifically, what technique do we use to calculate this final gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.rand(1, requires_grad=True)\n",
    "\n",
    "print(\"x:\", x, \"\\n\")\n",
    "\n",
    "z = x.log()\n",
    "\n",
    "y = z**2\n",
    "\n",
    "print(\"y:\", y, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by printing `y`, we can see that the tensor has a specific gradient function attached.\n",
    "\n",
    "Let us now compute the gradient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"gradient of x:\", x.grad, \"\\nQ:(gradient of x w.r.t. what?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us access $\\partial g/\\partial z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complicated example\n",
    "\n",
    "![](imgs/02/compgra3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "x_2 = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "print(\"x_1:  \", x_1)\n",
    "print(\"x_2:  \", x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct `c`, calculate the gradient and access it for both `x_1` and `x_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient accumulation\n",
    "\n",
    "Let us see another feature of torch differentiation functionalities.\n",
    "\n",
    "We can call `backward()` multiple times; let us see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## repeat the computation for c..\n",
    "c.backward()\n",
    "print(x_1.grad, x_2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: what is happening? Why the gradient is not the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom, non-parametric PyTorch module\n",
    "\n",
    "Basically, we want to create a module which is not controlled by any parameter, be it trainable or non-trainable.\n",
    "\n",
    "As an example, we might have the **Leaky ReLU**, an activation function which can be used in place of the more-known ReLU.\n",
    "\n",
    "$\\text{LeakyReLU} = \\max\\{0.01\\cdot x, x\\}$\n",
    "\n",
    "![](https://i1.wp.com/clay-atlas.com/wp-content/uploads/2019/10/image-37.png?resize=640%2C480&ssl=1)\n",
    "\n",
    "We can construct it like a basic PyTorch module, analogously to the MultiLayer Perceptron which we built (but not trained) at the end of Lab 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return torch.max(data, data*0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and that's it. We may plug it into a neural network module and it'll work just fine, both for the forward and backward pass.\n",
    "\n",
    "If we want, we can also use it as-is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = LeakyReLU()\n",
    "\n",
    "leaky_relu(torch.arange(-10,10)) # is identical to leaky_rely.forward(torch.arange(-10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us test its autodiff functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, -1.0], requires_grad=True)\n",
    "\n",
    "y = leaky_relu(x).sum() # sum to get one single value out of it.\n",
    "\n",
    "print(\"y:\", y)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"dy/dx:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the gradient gets calculated automatically without our intervention in defining a gradient function.\n",
    "\n",
    "But what if that was not already implemented in PyTorch? What if we needed to use some function that cannot be constructed by using PyTorch built-ins?\n",
    "\n",
    "In this case, we must define a function class which inherits from `torch.autograd.Function`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autograd Function inherits from `torch.autograd.Function` and has two compulsory methods: `forward` and `backward`, whose meaning should be obvious to all.\n",
    "\n",
    "Both functions have a compulsory first argument which is the **context**, `ctx` for brevity.\n",
    "From the context we can infer informations about the entities involved in the calculation of the gradient.\n",
    "\n",
    "The context is **built upon calling the `forward` method**, so that, during the `backward` call, we can obtain the info such what tensors have been used in `forward` and whether a tensor requires or not the grad.\n",
    "\n",
    "In our case, the derivative is the following:\n",
    "$\\frac{\\partial\\text{LeakyReLU}}{\\partial x} = \\begin{cases} 1\\text{ if }x>0 \\\\ 0\\text{ if }x\\leq 0\\end{cases}$, so we only need to save $x$, i.e., the data coming into the module.\n",
    "\n",
    "Moreover, the backward method needs an additional argument, `output_grad`, which conveys information about the gradient which is _entering_ the Function (be mindful, we're running _backward_, so a gradient _enters the function_ upstream w.r.t. the forward pass).\n",
    "\n",
    "This is necessary in order to build a cascade of sequential module, each applied after the other. This calls for the application of the **chain rule** for the computation of the gradient of **compositions of functions**:\n",
    "\n",
    "$$\n",
    "z = g(f(x)): \\\\\n",
    "y = f(x) \\wedge z = g(y) \\\\\n",
    "$$\n",
    "\n",
    "![](imgs/02/compgra_forward.jpg)\n",
    "\n",
    "Then, switching to the derivative:\n",
    "\n",
    "$\\Rightarrow \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot\\frac{\\partial y}{\\partial x} $\n",
    "\n",
    "![](imgs/02/compgra_backward2.gif)\n",
    "\n",
    "So, it becomes immediately overt the necessity of having an **incoming** gradient which you use to multiply with the gradient produced by the current module, the result of which gets passed on to the previous node in the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU_Fun(torch.autograd.Function):\n",
    "    @staticmethod # mind the decorator\n",
    "    def forward(ctx, input_):\n",
    "        ctx.save_for_backward(input_) # the parameters that will be involved in the gradient\n",
    "        return torch.max(input_, input_ * 0.01)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_, = ctx.saved_tensors # these are the variables which we need to backpropagate the gradient to (only the input)\n",
    "        # the gradient is 1 for positive x's, 0.01 for negative x's\n",
    "        grad_input = torch.ones_like(input_)\n",
    "        grad_input[input_<0] = 0.01\n",
    "        # now, we need to rescale for the grad_output\n",
    "        grad_input *= grad_output\n",
    "        '''\n",
    "        a valid alternative (maybe better performing?):\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input_<0] *= 0.01\n",
    "        '''\n",
    "        return grad_input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = LeakyReLU_Fun.apply\n",
    "x = torch.linspace(-5,5,11, requires_grad=True)\n",
    "y = fun(x)\n",
    "z = y.sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us then rivisit our `LeakyReLU` module from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU_Better(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return LeakyReLU_Fun.apply(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeakyReLU_Better()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom parametric module\n",
    "\n",
    "We wish to extend our Leaky ReLU module to the Parametric ReLU: $\\text{ParamReLU} = \\max\\{\\alpha\\cdot x, x\\}, \\alpha \\in [0,1)$.\n",
    "\n",
    "![](https://pytorch.org/docs/stable/_images/PReLU.png)\n",
    "\n",
    "Parametric ReLU with $\\alpha=0.25$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamReLU_Fun(torch.autograd.Function):\n",
    "    @staticmethod # mind the decorator\n",
    "    def forward(ctx, input_, alpha:float):\n",
    "        assert alpha >= 0 and alpha < 1, f\"alpha should be >= 0 and < 1. Found {alpha}.\"\n",
    "        ctx.save_for_backward(input_) # the parameters that will be involved in the gradient\n",
    "        ctx.alpha = alpha # note that we don't use self.alpha\n",
    "        return torch.max(input_, input_ * alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_, = ctx.saved_tensors # these are the variables which we need to backpropagate the gradient to (only the input)\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input_<0] *= ctx.alpha\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamReLU(torch.nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return ParamReLU_Fun.apply(X, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prelu = ParamReLU(0.25)\n",
    "x = torch.linspace(-5,5,11, requires_grad=True)\n",
    "y = prelu(x)\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered:\n",
    "\n",
    "1. The construction of a non-parametric differentiable module\n",
    "2. The construction of a parametric, non-trainable, differentiable module\n",
    "\n",
    "What's missing?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b77c29885b974dcfddc958349edd2398568367665d9f0a08534c65bb7a1b37b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "8aec2e4cca6a43ecda9b11f31ea0f9f4b012d28e6de8cbdf64a5e136ca9a5fb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
